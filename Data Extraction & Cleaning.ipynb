{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Alternative Medicine Finder - Application\n",
        "\n",
        "This project aims to help people find alternative medicines with the same ingredients. It's a common issue when visiting pharmacies - you might need a medicine, but it's not available. Though pharmacists can suggest alternatives, having a platform to search for medicines and find alternatives would be really helpful.\n",
        "\n",
        "The Project has been divided into 4 steps.\n",
        "\n",
        "1 - Data Extraction\n",
        "\n",
        "2 - Data Cleaning\n",
        "\n",
        "3 - Data Base Development\n",
        "\n",
        "4 - Developing an UI"
      ],
      "metadata": {
        "id": "FK9DrgVMnNBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Data Extraction\n",
        "\n",
        "In the first step, we'll use BeautifulSoup, a tool for web scraping, to extract the data we need. This will help us create a file with the information we need."
      ],
      "metadata": {
        "id": "OkJ-TzuQFv3D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBR-UsAQmfi3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def scrape_links(url):\n",
        "    extracted_links = []\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            h2_tags = soup.find_all('h2')\n",
        "            for h2 in h2_tags:\n",
        "                links = h2.find_all('a')\n",
        "                for link in links:\n",
        "                    extracted_links.append(link.get('href'))\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "    return extracted_links\n",
        "\n",
        "def fetch_and_store_data(url_list):\n",
        "    with open('extractedlinks.csv', 'w', newline='') as csvfile:\n",
        "        fieldnames = ['URL']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for url in url_list:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                extracted_links = scrape_links(response.url)\n",
        "                for extracted_link in extracted_links:\n",
        "                    writer.writerow({'URL': extracted_link})\n",
        "\n",
        "def extract_info_from_link(link):\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            # Extracting name from <h1> heading\n",
        "            h1_heading = soup.find('h1')\n",
        "            if h1_heading:\n",
        "                name_h1 = h1_heading.text.strip()\n",
        "            else:\n",
        "                name_h1 = \"N/A\"\n",
        "\n",
        "            # Extracting name field of <div class=\"d flex-column\">\n",
        "            div_d = soup.find('div', class_='d flex-column')\n",
        "            if div_d:\n",
        "                a_tag = div_d.find('a', class_='d block')\n",
        "                if a_tag:\n",
        "                    name_a_d_text = a_tag.text.strip()\n",
        "                else:\n",
        "                    name_a_d_text = \"N/A\"\n",
        "            else:\n",
        "                name_a_d_text = \"N/A\"\n",
        "\n",
        "            return name_h1, name_a_d_text\n",
        "        else:\n",
        "            print(f\"Failed to fetch {link}. Status code: {response.status_code}\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {link}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    # Generate list of URLs from 'a' to 'z'\n",
        "    base_url = \"https://dawaai.pk/all-medicines/\"\n",
        "    url_list = [base_url + chr(ord('a') + i) for i in range(26)]  # Generating URLs from 'a' to 'z'\n",
        "\n",
        "    # Fetch and store data\n",
        "    fetch_and_store_data(url_list)\n",
        "\n",
        "    input_csv = \"extractedlinks.csv\"\n",
        "    output_csv = \"output_data.csv\"\n",
        "\n",
        "    with open(input_csv, 'r') as f_input, open(output_csv, 'w', newline='') as f_output:\n",
        "        csv_reader = csv.reader(f_input)\n",
        "        csv_writer = csv.writer(f_output)\n",
        "\n",
        "        # Writing header for output CSV\n",
        "        csv_writer.writerow(['Link', 'Medicine', 'Medicinal Drug'])\n",
        "\n",
        "        for row in csv_reader:\n",
        "            link = row[0]  # Assuming the link is in the first column of the CSV\n",
        "            name_h1, name_a_d = extract_info_from_link(link)\n",
        "            csv_writer.writerow([link, name_h1, name_a_d])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code generates a CSV file containing information about medicines, including their names and the drugs they contain and a link leading to that medicine. There are three columns in total (Link, Medicine, Medicinal Drug). We've collected data from one website, resulting in 17,000 entries."
      ],
      "metadata": {
        "id": "XnZN8EEOEse1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Data Cleaning Process\n",
        "\n",
        "The output_data.csv file provides us with 3 columns, we need to get rid of the links column and then clean the data in the other 2 columns. We need to remove any unwanted characters. Additionally we also have to remove the duplicates present in the data to get out final output file. Please  refer to the below code to do all the steps explained above"
      ],
      "metadata": {
        "id": "WYf1oC7cGD7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# If the file is in the same directory you can use it directly othwerwiswe you can upload it\n",
        "\n",
        "# Upload CSV files from local system\n",
        "print(\"Upload CSV files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "mdata= pd.read_csv('output_data.csv')\n",
        "mdata= mdata.drop(columns=['Link'])\n",
        "\n",
        "\n",
        "#separate the medicine names from any non required information (Modify the code according to your data)\n",
        "#In the below line, any string part starting with a number is being removed.\n",
        "mdata['Medicines'] = mdata['Medicines'].str.replace(r'\\d.*$', '', regex=True)\n",
        "\n",
        "#same is done for the Medicinal Drug column\n",
        "mdata['Drugs'] = mdata['Drugs'].str.replace(r'\\(.*\\)', '', regex=True)\n",
        "\n",
        "#drop/remove any duplicate values\n",
        "mdata= mdata.drop_duplicates()\n",
        "\n",
        "# Export combined DataFrame to a CSV file\n",
        "mdata.to_csv('output_data_final.csv', index=False)"
      ],
      "metadata": {
        "id": "ZjtIfOgHF17o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0R28ZXuGKaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}